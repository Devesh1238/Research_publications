# ===================================================================== 
# SCRIPT: 03_run_pubmed_smart_query_v2_http_EXPERT.R 
# PURPOSE: High-precision, auditable PubMed linkage for NPPES surgeons 
# ARCH:    Tiered candidate generation + adaptive verification scoring 
# AUTHOR:  Code Copilot (compact, resilient, reproducible) 
# ===================================================================== 

suppressPackageStartupMessages({ 
  library(data.table) 
  library(httr) 
  library(jsonlite) 
  library(digest) 
  library(stringdist) 
}) 

`%||%` <- function(a,b) if (is.null(a) || length(a)==0) b else a

# -------------------------- CONFIG ----------------------------------- 
TOOL  <- "nppes_pubmed_pipeline" 
EMAIL <- "deveshrk@stanford.edu" 
# ---- HTTP UA + Authors API endpoints (PATCH) ---- 
UA <- sprintf("%s (mailto:%s)", TOOL, EMAIL)

U_A_STR <- UA
# Official Authors API (paper: /research/bionlp/APIs/authors/) 
AUTHORS_API <- "https://www.ncbi.nlm.nih.gov/research/litsense-api/api/author/" 

# Optional: slow down specific services if you later call Citation Matcher 
CITMATCH_DELAY_SEC <- 0.35  # 3 req/sec max; no concurrency per NLM guidance 

# -------------------------- PROJECT PATHS ------------------------------ 
# Assume your working directory is the RStudio Project root: 
PROJECT_DIR <- normalizePath(getwd(), winslash = "/", mustWork = FALSE) 

# Default subfolders that actually exist in your project 
DATA_DIR <- "01_data_raw" 
OUT_DIR  <- "outputs" 

# Allow env vars to override, but default to *existing* folders above 
IN_CSV   <- Sys.getenv("NPPES_IN", 
                       unset = file.path(DATA_DIR, "surgeons_pilot_100_for_query.csv")) 
OUT_CSV  <- Sys.getenv("NPPES_OUT", 
                       unset = file.path(OUT_DIR,  "surgeons_pilot_100_for_query_PUBCOUNTS.csv")) 
CACHE_DIR <- Sys.getenv("NPPES_CACHE", 
                        unset = file.path(OUT_DIR, "eutils_cache")) 
LOG_FILE  <- Sys.getenv("NPPES_LOG", 
                        unset = file.path(OUT_DIR, "phase2_log.txt")) 
# Helper: resolve relative → absolute under PROJECT_DIR; create parent if asked 
.is_abs_path <- function(p) { 
  grepl("^[A-Za-z]:[/\\\\]", p) || startsWith(p, "/") || startsWith(p, "\\\\") 
} 
resolve_path <- function(p, create_parent = FALSE, must_exist = FALSE) { 
  if (!nzchar(p)) return(p) 
  if (!.is_abs_path(p)) p <- file.path(PROJECT_DIR, p) 
  p_abs <- normalizePath(p, winslash = "/", mustWork = FALSE) 
  if (create_parent) { 
    dir.create(dirname(p_abs), recursive = TRUE, showWarnings = FALSE) 
  } 
  if (must_exist && !file.exists(p_abs)) { 
    stop("File not found: ", p_abs) 
  } 
  p_abs 
} 

API_KEY <- Sys.getenv("NCBI_API_KEY")  # correct: read from env var NCBI_API_KEY 
API_DELAY_SEC <- if (nchar(API_KEY) > 0) 0.12 else 0.35 
MAX_PMIDS_TO_FETCH <- 300L 
EFETCH_BATCH       <- 150L 

LOOKBACK_YEARS    <- 25L 
DATE_CEILING_YEAR <- as.integer(format(Sys.Date(), "%Y")) 
ROW_TIME_BUDGET   <- 180  # seconds 
RETRY_MAX  <- 5
RETRY_BASE <- 0.6
# ---------------- TIER ORCHESTRATION FLAGS ----------------
T5_ENABLED      <- TRUE          # turn on post-verification Authors API
USE_T5_ORCID    <- FALSE         # you said you don't have ORCIDs; keep as FALSE
T5_SUBBUDGET_S  <- 20            # per-row safety cap for T5 work

.VERY_COMMON_LAST <- toupper(c(
  "SMITH","JOHNSON","WILLIAMS","BROWN","JONES","MILLER","DAVIS","GARCIA",
  "RODRIGUEZ","MARTINEZ","HERNANDEZ","LOPEZ","GONZALEZ","WILSON","ANDERSON",
  "TAYLOR","THOMAS","MOORE","JACKSON","MARTIN","LEE","GUPTA"
))
# -------------------------- LOGGING ---------------------------------- 
log_line <- function(...) { 
  msg <- paste0(format(Sys.time(), "%F %T"), " | ", paste(..., collapse=" ")) 
  cat(msg, "\n") 
  try(write(msg, file = LOG_FILE, append = TRUE), silent = TRUE) 
} 

# --------------------------- CACHE ----------------------------------- 
.safe_name <- function(x) gsub("[^A-Za-z0-9._-]+", "_", x) 
cache_key  <- function(prefix, ...) .safe_name(paste(prefix, digest(list(...), algo="sha1"), sep = "__")) 
cache_path <- function(key) file.path(CACHE_DIR, paste0(key, ".rds")) 
cache_get  <- function(key) { p <- cache_path(key); if (file.exists(p)) readRDS(p) else NULL } 
cache_put  <- function(key, val) saveRDS(val, cache_path(key)) 

# right after you set CACHE_DIR and LOG_FILE:
CACHE_DIR <- resolve_path(CACHE_DIR)
dir.create(CACHE_DIR, recursive = TRUE, showWarnings = FALSE)
LOG_FILE  <- resolve_path(LOG_FILE, create_parent = TRUE)

# harden cache_put
cache_put <- function(key, val) {
  dir.create(CACHE_DIR, recursive = TRUE, showWarnings = FALSE)
  saveRDS(val, cache_path(key))
}
# --------------------------- HTTP ------------------------------------ 
BASE_URL <- "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/" 
ESEARCH  <- paste0(BASE_URL, "esearch.fcgi") 
EFETCH   <- paste0(BASE_URL, "efetch.fcgi") 

http_get_json <- function(url, query) { 
  attempt <- 0 
  query$tool  <- TOOL; query$email <- EMAIL 
  if (nchar(API_KEY) > 0) query$api_key <- API_KEY 
  key <- cache_key("GET_JSON", url, query) 
  if (!is.null(res <- cache_get(key))) return(res) 
  
  repeat { 
    attempt <- attempt + 1 
    Sys.sleep(API_DELAY_SEC) 
    
    resp <- try(httr::GET(url, 
                          query = query, 
                          httr::user_agent(sprintf("%s (mailto:%s)", TOOL, EMAIL)), 
                          httr::timeout(60)), silent = TRUE) 
    ok <- (!inherits(resp, "try-error")) && (!httr::http_error(resp)) 
    if (ok) { 
      out <- jsonlite::fromJSON(httr::content(resp, as = "text", encoding = "UTF-8"), 
                                simplifyVector = TRUE) 
      cache_put(key, out) 
      return(out) 
    } 
    
    if (attempt >= RETRY_MAX) { 
      qtxt <- tryCatch({ 
        if (!is.null(query$term)) paste0(" | term=", substr(query$term, 1, 300)) else "" 
      }, error = function(e) "") 
      stop(sprintf("GET_JSON failed: %s%s", 
                   if (inherits(resp, "try-error")) as.character(resp) 
                   else paste("HTTP", httr::status_code(resp)), 
                   qtxt)) 
    } 
    Sys.sleep(RETRY_BASE * (2^(attempt - 1))) 
  } 
} 

http_get_text <- function(url, query) { 
  attempt <- 0 
  query$tool  <- TOOL; query$email <- EMAIL 
  if (nchar(API_KEY) > 0) query$api_key <- API_KEY 
  key <- cache_key("GET_TEXT", url, query) 
  if (!is.null(res <- cache_get(key))) return(res) 
  
  repeat { 
    attempt <- attempt + 1 
    Sys.sleep(API_DELAY_SEC) 
    
    resp <- try(httr::GET(url, 
                          query = query, 
                          httr::user_agent(sprintf("%s (mailto:%s)", TOOL, EMAIL)), 
                          httr::timeout(60)), silent = TRUE) 
    ok <- (!inherits(resp, "try-error")) && (!httr::http_error(resp)) 
    if (ok) { 
      txt <- httr::content(resp, as = "text", encoding = "UTF-8") 
      cache_put(key, txt) 
      return(txt) 
    } 
    if (attempt >= RETRY_MAX) 
      stop(sprintf("GET_TEXT failed: %s", 
                   if (inherits(resp, "try-error")) as.character(resp) 
                   else paste("HTTP", httr::status_code(resp)))) 
    Sys.sleep(RETRY_BASE * (2^(attempt-1))) 
  } 
} 

http_post_text <- function(url, body) {
  attempt <- 0
  body$tool  <- TOOL
  body$email <- EMAIL
  if (nchar(API_KEY) > 0) body$api_key <- API_KEY
  key <- cache_key("POST_TEXT", url, body)
  if (!is.null(res <- cache_get(key))) return(res)
  
  repeat {
    attempt <- attempt + 1
    Sys.sleep(API_DELAY_SEC)
    resp <- try(httr::POST(
      url,
      body   = body,
      encode = "form",
      httr::user_agent(sprintf("%s (mailto:%s)", TOOL, EMAIL)),
      httr::timeout(60)
    ), silent = TRUE)
    
    ok <- (!inherits(resp, "try-error")) && (!httr::http_error(resp))
    if (ok) {
      txt <- httr::content(resp, as = "text", encoding = "UTF-8")
      cache_put(key, txt)
      return(txt)
    }
    if (attempt >= RETRY_MAX) {
      stop(sprintf("POST_TEXT failed: %s",
                   if (inherits(resp, "try-error")) as.character(resp)
                   else paste("HTTP", httr::status_code(resp))))
    }
    Sys.sleep(RETRY_BASE * (2^(attempt - 1)))
  }
}

# -------------------------- EUTILS ----------------------------------- 
# Append a date filter unless caller already specified any [dp]/[crdt]/[edat]/[pdat] 
.add_date_filter_if_missing <- function(term, years_lookback = LOOKBACK_YEARS) {
  if (!nzchar(term)) return(term)
  if (grepl("\\[(crdt|dp|edat|pdat)\\]", term, ignore.case = TRUE)) return(term)
  if (grepl("\\[\\s*Date\\s*-\\s*Publication\\s*\\]", term, ignore.case = TRUE)) return(term)
  sprintf("(%s) AND ((\"last %d years\"[crdt]) OR (\"last %d years\"[dp]))",
          term, years_lookback, years_lookback)
}
# top-level helper (near other utils)
is_valid_orcid <- function(x) {
  s <- gsub("-", "", toupper(trimws(as.character(x %||% ""))))
  if (!grepl("^[0-9]{15}[0-9X]$", s)) return(FALSE)
  total <- 0L
  for (i in 1:15) total <- (total + as.integer(substr(s, i, i))) * 2L
  remainder <- total %% 11L
  result <- (12L - remainder) %% 11L
  check <- if (result == 10L) "X" else as.character(result)
  identical(check, substr(s, 16, 16))
}

esearch_pubmed <- function(term, retmax = MAX_PMIDS_TO_FETCH) { 
  if (length(term) == 0L || is.na(term) || !nzchar(term)) { 
    return(list(ids = character(0), count = 0L)) 
  } 
  term <- .add_date_filter_if_missing(term, LOOKBACK_YEARS) 
  res <- http_get_json(ESEARCH, list(db="pubmed", term=term, retmax=retmax, retmode="json")) 
  if (is.null(res$esearchresult)) 
    return(list(ids=character(0), count=0L)) 
  list(ids = res$esearchresult$idlist %||% character(0), 
       count = as.integer(res$esearchresult$count %||% 0L)) 
}

efetch_medline_by_ids <- function(id_chr, chunk_size = EFETCH_BATCH) {
  ids <- unique(as.character(id_chr %||% character(0)))
  if (!length(ids)) return("")
  # helper that tries one chunk and on HTTP failure will error up
  fetch_chunk <- function(chunk) {
    tryCatch({
      http_get_text(EFETCH, list(db = "pubmed", id = paste(chunk, collapse = ","), rettype = "medline", retmode = "text"))
    }, error = function(e) stop(e))
  }
  # split into chunks and fetch, but if a chunk fails try smaller splits
  parts <- character(0)
  chunks <- split(ids, ceiling(seq_along(ids) / as.integer(chunk_size)))
  for (chunk in chunks) {
    ok <- FALSE
    attempt_size <- min(as.integer(chunk_size), length(chunk))
    while (!ok && attempt_size >= 1) {
      sub_chunks <- split(chunk, ceiling(seq_along(chunk) / attempt_size))
      # try all sub_chunks sequentially, will error if a single sub_chunk fails
      for (sc in sub_chunks) {
        txt <- try(fetch_chunk(sc), silent = TRUE)
        if (inherits(txt, "try-error")) {
          # shrink attempt_size and retry outer loop
          attempt_size <- max(1, floor(attempt_size / 2))
          break
        } else {
          parts <- c(parts, txt)
          ok <- TRUE
        }
      }
      if (!ok && attempt_size == 1) stop("efetch_medline_by_ids: failed even with single-id fetch (network / server error)")
    }
  }
  paste(parts, collapse = "\n\n")
}

# --- PATCH 2: MEDLINE parser (compact, robust) --- 
parse_medline_records <- function(medline_txt) { 
  if (is.null(medline_txt) || medline_txt == "") return(list()) 
  recs <- strsplit(medline_txt, "\n\n", fixed = TRUE)[[1]]
  recs <- recs[nzchar(recs)]
  lapply(recs, function(rec) {
    lines <- strsplit(rec, "\n", fixed = TRUE)[[1]]
    if (!length(lines) || all(!nzchar(lines))) return(NULL)
    tag <- substr(lines, 1, 4) 
    val <- sub("^....\\s*-\\s*", "", lines) 
    list( 
      PMID = val[tag == "PMID"], 
      AU   = val[tag == "AU  "], 
      FAU  = val[tag == "FAU "], 
      AD   = val[tag == "AD  "], 
      JT   = val[tag == "JT  "] 
    ) 
  }) 
} 
# --- T0-LENIENT VERIFIER: accept if AU has canonical LAST + first initial (no affil required) --- 
.canon_last_only <- function(x) { x <- toupper(x %||% ""); gsub("[^A-Z]", "", x) } 

# ----------------------- NORMALIZATION -------------------------------- 
#----Below is the start of normalization before new API------------------- 
up <- function(x) toupper(x %||% "") 
trim_c <- function(x) trimws(gsub("\\s+", " ", x %||% "")) 
canon_last <- function(x) { x <- up(x); x <- gsub("[’'`]+", "", x); gsub("[-\\s]+", "", x) } 
.sanitize_tok  <- function(x) trimws(gsub("\\s+", " ", as.character(x %||% ""))) 
.squash_scalar <- function(x) if (length(x)) as.character(x[[1]]) else "" 
# --- NEW: seed FAU fingerprint (T1 then T2) and reuse later --- 
seed_fingerprint_for_row <- function(row) { 
  last  <- up(row$last_name_norm) 
  first <- up(row$first_name_token) 
  mid   <- up(row$middle_name_token) 
  city  <- up(row$city_token); state <- up(row$state_token); org <- up(row$org_token) 
  
  fau_terms <- build_fau_terms(last, first, mid) 
  if (!length(fau_terms)) return(NULL) 
  
  # Try FAU + Affil, then FAU (no affil) 
  s1 <- esearch_pubmed_backoff(base_term = fau_terms[[1]], use_affil = TRUE,  field_used = "FAU", 
                               city_token = city, state_token = state, org_token = org, 
                               enum_date = row$EnumerationDate %||% NA_character_) 
  s  <- if (isTRUE(s1$ok)) s1 else 
    esearch_pubmed_backoff(base_term = fau_terms[[1]], use_affil = FALSE, field_used = "FAU", 
                           city_token = "", state_token = "", org_token = "", 
                           enum_date = row$EnumerationDate %||% NA_character_) 
  if (!isTRUE(s$ok)) return(NULL) 
  
  ids <- head(s$ids, EFETCH_BATCH) 
  med <- efetch_medline_by_ids(ids) 
  recs <- parse_medline_records(med) 
  if (!length(recs)) return(NULL) 
  
  fp <- build_fingerprint(recs, last, city, state, org) 
  list(fp = fp, seed_term = s$term_used) 
} 
# ===== T0 helpers (define BEFORE per-row loop) ======================= 
# --- A.1: helpers to build safe AU anchor terms (quoted to avoid truncation) --- 
.phrase <- function(x) sprintf("\"%s\"", gsub("\"", "", trimws(x))) 
.au_exact <- function(last_up, fi_up) sprintf("%s[au]", .phrase(sprintf("%s %s", last_up, fi_up))) 
.au_exact_fm <- function(last_up, fi_up, mi_up) sprintf("%s[au]", .phrase(sprintf("%s %s%s", last_up, fi_up, mi_up))) 

# === T5 switches & debug (drop-in) ===
if (!exists("T5_ENABLED", inherits = TRUE)) T5_ENABLED <- TRUE
if (!exists("T5_SUBBUDGET_S", inherits = TRUE)) T5_SUBBUDGET_S <- 8L
if (!exists("T5_DEBUG", inherits = TRUE)) T5_DEBUG <- TRUE
.log_t5 <- function(...) if (isTRUE(T5_DEBUG)) try(log_line("[T5]", ...), silent = TRUE)

run_t5_authors_api <- function(row, ans_core, t5_endpoint = "litsense") {
  start_t <- Sys.time()
  out <- list(
    t5_ultra_n     = 0L,
    pmids_t5_ultra = character(0),
    t5_seed_pmid   = "",
    t5_mode        = "anchor",
    t5_endpoint    = t5_endpoint,
    t5_reason      = "skipped"
  )
  
  raw_n <- as.integer(ans_core$raw_n %||% ans_core$fetch_primary_raw_n %||% length(ans_core$pmids_raw) %||% 0L)
  if (raw_n < 1L) { out$t5_reason <- "no_raw_ids"; return(out) }
  .log_t5("gate ok; raw_n=", raw_n)
  
  # Prefer exact string from your sheet if present; else compute variants
  names_vec <- character(0)
  if (!is.null(row[["last name and initial"]])) {
    nli <- as.character(row[["last name and initial"]][1] %||% "")
    if (nzchar(nli)) names_vec <- c(names_vec, nli)
  }
  names_vec <- unique(c(names_vec, name_forms_for_row(row)))
  if (!length(names_vec)) { out$t5_reason <- "no_names"; return(out) }
  
  seed <- as.character((ans_core$pmids_verified %||% ans_core$pmids_raw %||% character(0))[1] %||% "")
  if (!nzchar(seed)) {
    seed <- try(choose_seed_pmid(row), silent = TRUE)
    seed <- if (inherits(seed, "try-error")) "" else as.character(seed %||% "")
    out$t5_mode <- "anchor_rescue_probe"
  }
  if (!nzchar(seed)) { out$t5_reason <- "no_seed"; return(out) }
  out$t5_seed_pmid <- seed
  
  # Build "PMID NAME" strings – take up to 2 (keeps your semantics)
  pmid_name_queries <- sprintf("%s %s", seed, head(names_vec, 2L))
  .log_t5("T5 calling LitSense EXACT with: ", paste(pmid_name_queries, collapse = " | "))
  
  # 1) FIRST: use the exact-working path with cache OFF
  exact <- authors_api_fetch_litsense_exact(pmid_name_queries, use_cache = FALSE)
  
  if (identical(exact$reason, "ok") && length(exact$pmids)) {
    ultra <- unique(as.character(exact$pmids))
    out$pmids_t5_ultra <- ultra
    out$t5_ultra_n     <- length(ultra)
    out$t5_endpoint    <- exact$endpoint
    out$t5_reason      <- sprintf("ok_seed_from_T%d|exact", as.integer(ans_core$match_tier %||% NA_integer_))
  } else {
    .log_t5("Exact path empty/reason=", exact$reason %||% "unknown", " → try robust path")
    # 2) SECOND: use the robust multi-endpoint path (no-cache for safety)
    api_result <- authors_api_fetch(pmid_names = pmid_name_queries, use_cache = FALSE)
    if (identical(api_result$reason, "ok") && length(api_result$pmids)) {
      ultra <- unique(as.character(api_result$pmids))
      out$pmids_t5_ultra <- ultra
      out$t5_ultra_n     <- length(ultra)
      out$t5_endpoint    <- api_result$endpoint %||% "litsense"
      out$t5_reason      <- sprintf("ok_seed_from_T%d|robust", as.integer(ans_core$match_tier %||% NA_integer_))
    } else {
      out$t5_endpoint <- api_result$endpoint %||% exact$endpoint %||% t5_endpoint
      out$t5_reason   <- paste0("no_cluster (reason: ", api_result$reason %||% exact$reason %||% "unknown", ")")
    }
  }
  
  if (!exists("T5_SUBBUDGET_S", inherits = TRUE)) T5_SUBBUDGET_S <<- 3
  if (as.numeric(difftime(Sys.time(), start_t, units = "secs")) > T5_SUBBUDGET_S) {
    out$t5_reason <- paste0(out$t5_reason, "|time_cap")
  }
  out
}
.attach_t5_to_ans <- function(row, ans_core) {
  # safe defaults
  blank <- list(
    t5_ultra_n     = 0L,
    pmids_t5_ultra = character(0),
    t5_seed_pmid   = "",
    t5_mode        = "anchor",
    t5_endpoint    = "litsense",
    t5_reason      = "skipped"
  )
  t5 <- try(run_t5_authors_api(row, ans_core), silent = TRUE)
  if (inherits(t5, "try-error") || is.null(t5)) t5 <- blank
  # force types
  t5$t5_ultra_n     <- as.integer(t5$t5_ultra_n %||% 0L)
  t5$pmids_t5_ultra <- unique(as.character(t5$pmids_t5_ultra %||% character(0)))
  t5$t5_seed_pmid   <- as.character(t5$t5_seed_pmid %||% "")
  t5$t5_mode        <- as.character(t5$t5_mode %||% "anchor")
  t5$t5_endpoint    <- as.character(t5$t5_endpoint %||% "litsense")
  t5$t5_reason      <- as.character(t5$t5_reason %||% "skipped")
  c(ans_core, t5)
}

# === Public entrypoint ===
run_tiers <- function(row) {
  ans <- run_tiers_for_row(row)       # first tier with any raw hits wins
  ans <- .attach_t5_to_ans(row, ans)  # always run T5 after that
  ans
}
# Build the “LAST F” and optional “LAST FM” name forms (uppercased, no punctuation).
name_forms_for_row <- function(row) {
  last  <- toupper(.sanitize_tok(row$last_name_norm %||% ""))
  first <- toupper(.sanitize_tok(row$first_name_token %||% ""))
  mid   <- toupper(substr(.sanitize_tok(row$middle_name_token %||% ""), 1, 1))
  if (!nzchar(last) || !nzchar(first)) return(character(0))
  fi <- substr(first, 1, 1)
  forms <- c(sprintf("%s %s", last, fi))
  if (nzchar(mid)) forms <- c(forms, sprintf("%s %s%s", last, fi, mid))
  unique(forms)
}
# Choose a simple, deterministic seed PMID for T5.
choose_seed_pmid <- function(row, ans_core = NULL) {
  if (is.null(ans_core) && exists("..last_ans_core", inherits = TRUE)) ans_core <- ..last_ans_core
  if (!is.null(ans_core)) {
    s <- as.character((ans_core$pmids_verified %||% ans_core$pmids_raw %||% character(0))[1] %||% "")
    if (nzchar(s)) return(s)
  }
  # As a last resort, do nothing (run_t5_authors_api will treat as no_seed).
  NA_character_
}

# Adapter so T0 can reuse your existing fingerprint/accept rules 
verify_cluster_with_existing_rules <- function(recs, row) { 
  last  <- up(row$last_name_norm) 
  city  <- up(row$city_token); state <- up(row$state_token); org <- up(row$org_token) 
  fp <- build_fingerprint(recs, last, city, state, org) 
  if (!is.null(fp$accepted_pmids)) return(unique(fp$accepted_pmids)) 
  if (!is.null(fp$pmids_high_conf)) return(unique(fp$pmids_high_conf)) 
  character(0) 
} 
#--------------------HELPERS ADD HERE---------------------------------------- 
#-----------This HELPER is pulling API from 2024 paper----------------------- 
# Paper says: query is “last name + first initial” + any known PMID; multiple name variants comma-separated 
# Ref: straightforward retrieval & API location 
#  - Query strings: "<Last> <F>|<pmid>" and you can comma-join name variants  (paper) 

# Helper: normalize "Last, First Middle" → c(last="LAST", fi="F") 
.name_to_last_fi <- function(full_name) { 
  full_name <- trimws(gsub("\\s+", " ", full_name)) 
  # Accept "Last, First Middle" or "First Middle Last" 
  if (grepl(",", full_name)) { 
    parts <- strsplit(full_name, ",\\s*")[[1]] 
    last  <- toupper(trimws(parts[1])) 
    given <- trimws(parts[2]) 
  } else { 
    parts <- strsplit(full_name, "\\s+")[[1]] 
    last  <- toupper(tail(parts, 1)) 
    given <- paste(head(parts, -1), collapse = " ") 
  } 
  fi <- toupper(substr(given, 1, 1)) 
  list(last = last, fi = fi) 
} 

# Return vector of PMIDs from Authors API cluster 
# --- A.5: Authors API callers (UA included; robust JSON shape) --- 
.extract_pmids_anywhere <- function(obj) {
  # 1) Preferred: JSON shape from LitSense — list(results = [ { pmids = [ints], ... }, ... ])
  try_rs <- try(obj$results, silent = TRUE)
  if (!inherits(try_rs, "try-error") && is.list(try_rs) && length(try_rs)) {
    pmid_nums <- unlist(lapply(try_rs, function(r) r$pmids), use.names = FALSE)
    pmid_chr  <- as.character(pmid_nums)
    pmid_chr  <- pmid_chr[nzchar(pmid_chr)]
    pmid_chr  <- unique(pmid_chr[nchar(pmid_chr) >= 7])
    if (length(pmid_chr)) return(pmid_chr)
  }
  # 2) Fallback: look for any atomic leaf (numeric or character) and extract digit runs
  harvest <- rapply(
    obj,
    function(v) {
      if (!is.atomic(v)) return(NULL)
      vv <- as.character(v)
      regmatches(vv, gregexpr("\\b\\d{7,8}\\b", vv))
    },
    how = "unlist"
  )
  if (length(harvest)) {
    flat <- unique(unlist(harvest, use.names = FALSE))
    flat <- flat[nzchar(flat)]
    return(flat)
  }
  character(0)
}
# Exact LitSense caller, matching the standalone working code semantics
authors_api_fetch_litsense_exact <- function(pmid_names, timeout_sec = 30L, use_cache = FALSE) {
  # Build "PMID LAST FI[,PMID LAST FI]" exactly
  qstring <- paste(unique(pmid_names), collapse = ",")
  
  # Cache key *includes* the UA string to avoid poisoning across edits
  key <- cache_key("AUTHORS_API_LITSENSE_EXACT", qstring, "UA=R-Script/1.0")
  if (isTRUE(use_cache) && !is.null(cache_get(key))) return(cache_get(key))
  
  base_url <- "https://www.ncbi.nlm.nih.gov/research/litsense-api/api/author/"
  request_url <- httr::modify_url(base_url, query = list(query = qstring))
  
  # UA exactly like your working script
  resp <- try(httr::GET(request_url, httr::add_headers(`User-Agent` = "R-Script/1.0"),
                        httr::timeout(timeout_sec)), silent = TRUE)
  if (inherits(resp, "try-error")) {
    return(list(pmids = character(0), endpoint = base_url, reason = "http_error"))
  }
  if (httr::http_error(resp)) {
    return(list(pmids = character(0), endpoint = base_url,
                reason = paste0("http_", httr::status_code(resp))))
  }
  
  txt <- httr::content(resp, "text", encoding = "UTF-8")
  # Keep your “flatten=TRUE” path, then fall back to robust harvest
  parsed <- try(jsonlite::fromJSON(txt, flatten = TRUE), silent = TRUE)
  pmids <- character(0)
  if (!inherits(parsed, "try-error")) {
    if (!is.null(parsed$results) && nrow(parsed$results) > 0 && "pmids" %in% names(parsed$results)) {
      pmids <- as.character(parsed$results$pmids[[1]] %||% character(0))
    }
  }
  if (!length(pmids)) {
    # fallback to robust harvester used elsewhere in your code
    obj <- try(jsonlite::fromJSON(txt, simplifyVector = FALSE), silent = TRUE)
    if (!inherits(obj, "try-error")) pmids <- .extract_pmids_anywhere(obj)
  }
  pmids <- unique(as.character(pmids[nzchar(pmids)]))
  
  ans <- list(pmids = pmids, endpoint = base_url, reason = if (length(pmids)) "ok" else "empty")
  # IMPORTANT: cache only on success
  if (isTRUE(use_cache) && length(pmids)) cache_put(key, ans)
  ans
}

AUTHORS_API_CANDIDATES <- c(
  # Primary (documented) LitSense author resolver
  "https://www.ncbi.nlm.nih.gov/research/litsense-api/api/author/",
  # Backup: same service via BioNLP site; note the plural "authors" path
  "https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/authors/",
  # Legacy fallback (singular path still works on some mirrors)
  "https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/author/"
)
authors_api_fetch <- function(pmid_names = NULL, raw_query = NULL, timeout_sec = 30L, use_cache = TRUE) {
  if (!exists("U_A_STR", inherits = TRUE)) {
    U_A_STR <<- sprintf("%s (mailto:%s)", TOOL, EMAIL)
  }
  
  if (!length(pmid_names) && !nzchar(raw_query %||% "")) {
    return(list(pmids = character(0), endpoint = NA_character_, reason = "no_query"))
  }
  qstring <- if (length(pmid_names)) paste(unique(pmid_names), collapse = ",") else raw_query
  
  key <- cache_key("AUTHORS_API", qstring, paste0("UA=", U_A_STR))
  if (isTRUE(use_cache) && !is.null(cache_get(key))) return(cache_get(key))
  
  last_reason <- "no_response"
  for (ep in AUTHORS_API_CANDIDATES) {
    resp <- try(
      httr::GET(ep,
                query = list(query = qstring),
                httr::user_agent(U_A_STR),
                httr::accept_json(),
                httr::timeout(timeout_sec)),
      silent = TRUE
    )
    if (inherits(resp, "try-error")) { last_reason <- "http_error"; next }
    sc <- httr::status_code(resp)
    if (sc != 200) { last_reason <- paste0("http_", sc); next }
    
    txt <- httr::content(resp, as = "text", encoding = "UTF-8")
    obj <- try(jsonlite::fromJSON(txt, simplifyVector = FALSE), silent = TRUE)
    if (inherits(obj, "try-error")) { last_reason <- "json_error"; next }
    
    pmids <- .extract_pmids_anywhere(obj)
    pmids <- unique(as.character(pmids[nzchar(pmids)]))
    if (length(pmids)) {
      ans <- list(pmids = pmids, endpoint = ep, reason = "ok")
      if (isTRUE(use_cache)) cache_put(key, ans)  # cache only on success
      return(ans)
    } else {
      last_reason <- "empty"
    }
  }
  # do NOT cache errors/empties anymore
  list(pmids = character(0), endpoint = NA_character_, reason = last_reason)
}

authors_api_by_orcid <- function(orcid, timeout_sec = 20L) {
  if (!nzchar(orcid)) return(character(0))
  key <- cache_key("AUTHORS_BY_ORCID", orcid)
  if (!is.null(cache_get(key))) return(cache_get(key))
  pmids <- character(0)
  for (ep in AUTHORS_API_CANDIDATES) {
    resp <- try(httr::GET(
      ep,
      query = list(orcid = orcid),
      httr::user_agent(U_A_STR),  # <- fixed UA
      httr::accept_json(),
      httr::timeout(timeout_sec)
    ), silent = TRUE)
    if (inherits(resp, "try-error")) next
    if (httr::status_code(resp) != 200) next
    txt <- httr::content(resp, as = "text", encoding = "UTF-8")
    obj <- try(jsonlite::fromJSON(txt, simplifyVector = FALSE), silent = TRUE)
    if (inherits(obj, "try-error")) next
    pmids <- .extract_pmids_anywhere(obj)
    if (length(pmids)) break
  }
  cache_put(key, pmids)
  pmids
}

# Full T0 flow: 
#  1) pick a seed PMID (best-guess from any coarse Tier probe or fingerprint) 

#---------Above is end of Author API key-------------------------- 
# ---------- PATCH A: helpers ---------- 
is_very_common_last <- function(last_norm) { 
  L <- toupper(trimws(as.character(last_norm %||% ""))) 
  if (!nzchar(L)) return(FALSE) 
  # expand the very-common set (feel free to add more) 
  very <- c(.VERY_COMMON_LAST,  
            "LEE","KIM","PARK","CHEN","WANG","LI","SINGH","PATEL", 
            "MARTINEZ","RODRIGUEZ","GARCIA","BROWN","JACKSON", 
            "KAPLAN","PRINCE","WILSON","TAYLOR","MILLER","DAVIS") 
  L %in% unique(very) 
} 

has_rare_middle_initial <- function(middle_tok) { 
  mi <- toupper(substr(.sanitize_tok(middle_tok), 1, 1)) 
  if (!nzchar(mi)) return(FALSE) 
  # "rare" = not among the most frequent initials 
  !(mi %in% c("A","J","M","S","R","D","L","C","K","P")) 
} 

# robust AND joiner for building final PubMed term strings 
.and_join <- function(...) { 
  parts <- Filter(nzchar, trimws(c(...))) 
  if (!length(parts)) return("") 
  # ensure each conjunct is separated by single space 
  # and we inject AND between logical blocks 
  out <- parts[1] 
  if (length(parts) > 1) { 
    for (i in 2:length(parts)) out <- paste(out, "AND", parts[i]) 
  } 
  out 
} 
#-----------------NEXT Helper------------------------------------- 
augment_fau_once <- function(base_term, city, state, org, enum_date,  
                             exclude_term, already_ids) { 
  # Try FAU with/without affil flipped once (same date ladder) 
  aug <- list() 
  for (use_affil in c(TRUE, FALSE)) { 
    s <- esearch_pubmed_backoff(base_term, use_affil, "FAU", 
                                city, state, org, enum_date, 
                                very_common_last = FALSE) 
    if (!isTRUE(s$ok)) next 
    if (identical(s$term_used, exclude_term)) next 
    new_ids <- setdiff(s$ids, already_ids) 
    if (length(new_ids)) aug[[length(aug)+1]] <- list(term=s$term_used, ids=new_ids, used_affil=s$used_affil) 
  } 
  aug 
} 

.is_blankish <- function(x) { 
  z <- toupper(trimws(.squash_scalar(x))) 
  (!nzchar(z)) || z %in% c("NA","N/A","NULL","NONE","UNKNOWN","UNSPECIFIED",".","-") 
} 
# Treat as generic only if the entire string is a generic word, or a bare department/division line 
# --- NA-safe, anchored generic org filter (define ONCE) --- 
is_generic_org <- function(x) { 
  s <- .sanitize_tok(x)                    # length-1, never NA 
  if (!nzchar(s)) return(TRUE)             # empty => generic 
  S <- toupper(s) 
  if (is.na(S)) return(TRUE)               # belt & suspenders 
  
  # exact generic words only 
  if (grepl("^(HOSPITAL|CLINIC|MEDICAL CENTER|DEPARTMENT|DIVISION|UNIVERSITY)$", S)) return(TRUE) 
  
  # bare department/division lines w/ or w/o "OF ..." 
  if (grepl("^(DEPARTMENT|DIVISION)(\\s+OF\\b.*)?$", S)) return(TRUE) 
  
  if (nchar(S) <= 2L) return(TRUE)         # too short to be meaningful 
  
  FALSE                                     # keep named orgs (e.g., UCLA MEDICAL CENTER) 
} 
# ----------------------- AFFILIATION HELPERS -------------------------- 
# --- PATCH 1: NA-safe affiliation builder (no "NA"[Affiliation]) --- 
.is_bool <- function(x) { 
  # scalarize & guard 
  t <- toupper(as.character(x %||% "")[1]) 
  t %in% c("AND","OR","NOT") 
} 
.STATE_FULL <- c( 
  AL="ALABAMA", AK="ALASKA", AZ="ARIZONA", AR="ARKANSAS", CA="CALIFORNIA", 
  CO="COLORADO", CT="CONNECTICUT", DE="DELAWARE", FL="FLORIDA", GA="GEORGIA", 
  HI="HAWAII", ID="IDAHO", IL="ILLINOIS", IN="INDIANA", IA="IOWA", KS="KANSAS", 
  KY="KENTUCKY", LA="LOUISIANA", ME="MAINE", MD="MARYLAND", MA="MASSACHUSETTS", 
  MI="MICHIGAN", MN="MINNESOTA", MS="MISSISSIPPI", MO="MISSOURI", MT="MONTANA", 
  NE="NEBRASKA", NV="NEVADA", NH="NEW HAMPSHIRE", NJ="NEW JERSEY", NM="NEW MEXICO", 
  NY="NEW YORK", NC="NORTH CAROLINA", ND="NORTH DAKOTA", OH="OHIO", OK="OKLAHOMA", 
  OR="OREGON", PA="PENNSYLVANIA", RI="RHODE ISLAND", SC="SOUTH CAROLINA", 
  SD="SOUTH DAKOTA", TN="TENNESSEE", TX="TEXAS", UT="UTAH", VT="VERMONT", 
  VA="VIRGINIA", WA="WASHINGTON", WV="WEST VIRGINIA", WI="WISCONSIN", WY="WYOMING", 
  DC="DISTRICT OF COLUMBIA" 
) 
# ---- Step 4.2: tiny locality alias map (state-scoped) ---- 
.CITY_ALIASES <- list( 
  "PA:HAZLE TOWNSHIP" = c("HAZLE TOWNSHIP", "HAZLETON"), 
  "NY:NEW YORK"       = c("NEW YORK","NYC","MANHATTAN","BROOKLYN","QUEENS","BRONX","STATEN ISLAND"), 
  "NY:STATEN ISLAND"  = c("STATEN ISLAND","NEW YORK","NYC"), 
  "CA:LOS ANGELES"    = c("LOS ANGELES","LA"), 
  "CA:SAN FRANCISCO"  = c("SAN FRANCISCO","SF") 
) 
.city_aliases_for <- function(city_tok, state_tok) { 
  st_raw <- toupper(.sanitize_tok(state_tok %||% "")) 
  ct     <- toupper(.sanitize_tok(city_tok %||% "")) 
  if (!nzchar(ct)) return(character(0)) 
  
  # Derive both code and full for the state 
  st_code <- if (nzchar(st_raw) && nchar(st_raw) == 2L) st_raw else { 
    # try to map full -> code using .STATE_FULL 
    ix <- which(toupper(.STATE_FULL) == st_raw) 
    if (length(ix)) names(.STATE_FULL)[ix[1]] else st_raw 
  } 
  st_full <- if (nzchar(st_raw) && nchar(st_raw) == 2L && st_raw %in% names(.STATE_FULL)) { 
    .STATE_FULL[[st_raw]] 
  } else st_raw 
  
  keys <- unique(c( 
    if (nzchar(st_code)) paste0(st_code, ":", ct) else NULL, 
    if (nzchar(st_full)) paste0(st_full, ":", ct) else NULL 
  )) 
  
  out <- ct 
  for (k in keys) { 
    if (!is.null(.CITY_ALIASES[[k]])) { 
      out <- c(out, toupper(.CITY_ALIASES[[k]])) 
    } 
  } 
  unique(out) 
} 
.city_aliases <- function(city_token) { 
  c0 <- toupper(.sanitize_tok(city_token %||% "")) 
  if (!nzchar(c0)) return(character(0)) 
  unique(c(c0, .CITY_ALIASES[[c0]] %||% character(0))) 
} 
.state_aliases <- function(state_token) {
  st <- toupper(.sanitize_tok(state_token %||% ""))
  if (!nzchar(st)) return(character(0))
  full <- if (nchar(st) == 2L && st %in% names(.STATE_FULL)) .STATE_FULL[[st]] else st
  # DROP spaced "I L" form entirely
  unique(c(st, full))
}
# --- NEW: normalize to USPS 2-letter; fall back to uppercased input
.normalize_state_usps <- function(state_token) {
  s <- toupper(.sanitize_tok(state_token %||% ""))
  if (!nzchar(s)) return("")
  if (nchar(s) == 2L && s %in% names(.STATE_FULL)) return(s)
  ix <- which(toupper(.STATE_FULL) == s)
  if (length(ix)) return(names(.STATE_FULL)[ix[1]])
  s
}
AMBIG_STATE_EXTRA_USA <- c("IN","OR","ME","MS","PA","UT","HI","DC")

build_state_affil_block <- function(state_token) {
  usps <- .normalize_state_usps(state_token)
  if (!nzchar(usps)) return("")
  core <- sprintf('"%s"[ad]', usps)
  if (usps %in% AMBIG_STATE_EXTRA_USA) paste(core, 'AND "USA"[ad]') else core
}
# --- helper: NA-safe check for any affiliation inputs present
.has_any_affil <- function(city, state, org) {
  nz1 <- function(x) { z <- .sanitize_tok(x); isTRUE(nzchar(z)) }
  any(c(nz1(city), nz1(state), nz1(org)))
}
# --- FAU-only small-sample rescue used by run_tiers_for_row()
rescue_sample_fau <- function(row, sample_n = 50L) {
  last_norm <- up(row$last_name_norm); first_tok <- up(row$first_name_token)
  if (!nzchar(last_norm) || !nzchar(first_tok)) return(NULL)
  fau_terms <- build_fau_terms(last_norm, first_tok, row$middle_name_token)
  if (!length(fau_terms)) return(NULL)
  # try FAU without affil, short sample
  s <- try(esearch_pubmed(fau_terms[[1]], retmax = sample_n), silent = TRUE)
  if (inherits(s, "try-error") || !length(s$ids)) return(NULL)
  ids <- head(s$ids, sample_n)
  med <- try(efetch_medline_by_ids(ids, chunk_size = min(50L, length(ids))), silent = TRUE)
  if (inherits(med, "try-error") || !nzchar(med)) return(NULL)
  recs <- parse_medline_records(med)
  if (!length(recs)) return(NULL)
  # run lenient scoring (no affil requirement) using existing fingerprint logic
  fp <- build_fingerprint(recs, last_norm, row$city_token, row$state_token, row$org_token)
  scores <- lapply(recs, score_record,
                   last_norm = last_norm, target_first = first_tok, modal_first = fp$modal_first,
                   city = row$city_token, state = row$state_token, org = row$org_token,
                   top_co = fp$top_coauthors, top_jr = fp$top_journals)
  totals <- vapply(scores, function(s) s$total, numeric(1))
  name_sims <- vapply(scores, function(s) s$name, numeric(1))
  # choose modest thresholds for rescue: decent name OR composite
  keep <- which((name_sims >= 0.75) | (totals >= 0.65))
  if (!length(keep)) return(NULL)
  pmids <- unique(vapply(recs[keep], function(r) r$PMID[1], character(1)))
  list(ok = TRUE, pmids_verified = pmids, pmids_firstauthor = character(0), pmids_lastauthor = character(0),
       term_used = fau_terms[[1]])
}
# --- NEW: affiliation variants ladder for backoff --- 
# --- vector-safe normalizer that returns a CHARACTER VECTOR (possibly length 0) ---
.normq <- function(x) {
  x <- toupper(.sanitize_tok(x))
  if (length(x) == 0) return(character(0))
  x <- as.character(x)
  x <- x[nzchar(x)]
  if (!length(x)) return(character(0))
  sprintf('"%s"[ad]', x)   # keep affiliation tagging here; proximity added below
}

# Build proximity-based affiliation variants per NLM guidance
# ---- helpers for cartesian products that return LISTS of character vectors ----
.cartesian_pairs <- function(a, b) {
  a <- a[nzchar(a)]; b <- b[nzchar(b)]
  if (!length(a) || !length(b)) return(list())
  grd <- expand.grid(a = a, b = b, stringsAsFactors = FALSE)
  lapply(seq_len(nrow(grd)), function(i) c(as.character(grd$a[i]), as.character(grd$b[i])))
}
.cartesian_triples <- function(a, b, c_prefix) {
  pairs <- .cartesian_pairs(a, b)
  if (!length(pairs)) return(list())
  lapply(pairs, function(p) c(c_prefix, p))
}

# ---- small helpers used below (unchanged semantics) ----
.ad_or <- function(xs) {
  xs <- unique(toupper(xs[nzchar(xs)]))
  if (!length(xs)) return("")
  paste(sprintf('"%s"[ad]', xs), collapse = " OR ")
}
.prox_or <- function(parts_list, N) {
  if (!length(parts_list)) return("")
  clauses <- vapply(parts_list, function(terms) {
    terms <- unique(toupper(terms[nzchar(terms)]))
    sprintf('"%s"[ad:~%d]', paste(terms, collapse = " "), as.integer(N))
  }, character(1))
  paste(clauses, collapse = " OR ")
}

# ---- Build affiliation variants ----
build_affil_variants <- function(city_token, state_token, org_token) {
  # Goal: state-only (USPS + FULL) OR-block, plus a "none" option.
  # We intentionally drop city/org and all proximity logic.
  
  # 1) Collect state aliases (USPS + FULL) and filter out junk (e.g., "I L")
  raw <- unique(toupper(.state_aliases(state_token)))
  # keep only USPS codes that exist in the table OR exact FULL names
  keep <- raw[(raw %in% names(.STATE_FULL)) | (raw %in% toupper(unname(.STATE_FULL)))]
  
  # 2) Build a simple ("CA"[ad] OR "CALIFORNIA"[ad]) block, if anything remains
  state_block <- ""
  if (length(keep)) {
    state_block <- paste(sprintf('"%s"[ad]', keep), collapse = " OR ")
    state_block <- paste0("(", state_block, ")")
  }
  
  # 3) Return only two variants in the usual shape used by callers:
  #    - first item is the state OR-block (if present)
  #    - second item is a "none" (empty) block
  blocks <- list()
  push <- function(label, block) blocks[[length(blocks) + 1]] <<- list(block = block, label = label)
  
  if (nzchar(state_block)) push("state|OR", state_block)
  push("none", "")
  
  blocks
}
#-------------------newly added helper, can remove------------------------- 
.affil_country_penalty <- function(ad_vec) { 
  if (!length(ad_vec)) return(0) 
  ads <- toupper(paste(unlist(ad_vec), collapse = " | ")) 
  # penalty if clearly non-US 
  if (grepl("\\bCANADA\\b|\\bUNITED KINGDOM\\b|\\bAUSTRALIA\\b|\\bFRANCE\\b", ads)) return(-0.15) 
  0 
} 

# ---------- PATCH B: weighted affiliation score ---------- 
.affil_score <- function(ad_vec, city_token, state_token, org_token) {
  if (length(ad_vec) == 0) return(0)
  ads <- toupper(as.character(unlist(ad_vec)))
  if (requireNamespace("stringi", quietly = TRUE)) {
    ads <- stringi::stri_trans_general(ads, "Latin-ASCII")
  }
  norm <- function(x) { x <- .sanitize_tok(x); t <- toupper(x); gsub("[^A-Z0-9]+"," ", t) }
  re_word <- function(tok) {
    t <- norm(tok)
    if (!nzchar(t)) return(NA_character_)
    paste0("\\b", gsub(" ", "\\\\s+", t), "\\b")
  }
  st_full <- { st <- toupper(.sanitize_tok(state_token)); if (nzchar(st) && nchar(st) == 2L && st %in% names(.STATE_FULL)) .STATE_FULL[[st]] else st }
  re_city  <- re_word(city_token)
  re_state <- re_word(st_full)
  re_org   <- if (nzchar(.sanitize_tok(org_token)) && !is_generic_org(org_token)) re_word(org_token) else NA_character_
  has_re <- function(rex) {
    if (is.null(rex) || is.na(rex) || !nzchar(rex)) return(FALSE)
    any(grepl(rex, ads, perl = TRUE))
  }
  has_city  <- has_re(re_city)
  has_state <- has_re(re_state)
  has_org   <- has_re(re_org)
  base <- 0
  # gentler scoring: city+state strong, city-only moderate, state-only small but meaningful
  if (has_city && has_state) base <- 0.9
  else if (has_city)         base <- 0.55
  else if (has_state)        base <- 0.35
  else if (has_org)          base <- 0.25
  adj <- base + .affil_country_penalty(ad_vec)
  max(0, adj)
}
affil_score <- .affil_score
# ----------------------- DATE FILTER ---------------------------------- 
parse_enum_year <- function(enum_chr) { 
  if (is.null(enum_chr) || is.na(enum_chr) || !nzchar(enum_chr)) return(NA_integer_) 
  for (fmt in c("%m/%d/%Y", "%m/%d/%y", "%Y-%m-%d")) { 
    d <- try(as.Date(enum_chr, format=fmt), silent = TRUE) 
    if (!inherits(d,"try-error") && !is.na(d)) return(as.integer(format(d, "%Y"))) 
  } 
  suppressWarnings({ d2 <- as.Date(enum_chr); if (!is.na(d2)) return(as.integer(format(d2, "%Y"))) }) 
  NA_integer_ 
} 
# --- FIX: date windows carry no leading AND; add a wide rescue 2-step date ladder (25y, then 50y rescue) --- 
build_date_filters <- function(enum_chr) { 
  ey <- parse_enum_year(enum_chr) 
  ceiling <- DATE_CEILING_YEAR 
  start25 <- if (is.na(ey)) ceiling - 25L else max(1900L, ey - 25L) 
  start50 <- if (is.na(ey)) ceiling - 50L else max(1900L, ey - 50L) 
  c( 
    sprintf('("%d/01/01"[Date - Publication] : "%d/12/31"[Date - Publication])', start25, ceiling), 
    sprintf('("%d/01/01"[Date - Publication] : "%d/12/31"[Date - Publication])', start50, ceiling), 
    sprintf('("1950/01/01"[Date - Publication] : "%d/12/31"[Date - Publication])', ceiling)  # rescue 
  ) 
} 

# Bridge helpers for the probe functions 
build_date_filter <- function(enum_chr) build_date_filters(enum_chr)[1] 

build_affil_block <- function(city_token, state_token, org_token) { 
  v <- build_affil_variants(city_token, state_token, org_token) 
  if (length(v) && nzchar(v[[1]]$block)) v[[1]]$block else "" 
} 
# ----------------------- AUTHOR UTILITIES ----------------------------- 
first_initial <- function(x) { x <- up(x); if (nzchar(x)) substr(x,1,1) else "" } 

allowed_author_set <- function(last_norm, first_tok, middle_tok) { 
  L <- trim_c(last_norm); F <- trim_c(first_tok); M <- trim_c(middle_tok) 
  fi <- first_initial(F); mi <- first_initial(M) 
  unique(toupper(c(paste(L, F), paste(L, fi), paste(L, fi, mi), paste(L, paste0(fi, mi))))) 
} 

author_match_ok <- function(au_vec, fau_vec, allowed_norm) { 
  au_norm  <- toupper(trim_c(au_vec)) 
  fau_norm <- toupper(trim_c(fau_vec)) 
  if (length(au_norm) && any(au_norm %in% allowed_norm)) return(TRUE) 
  if (length(fau_norm) && any(startsWith(fau_norm, paste0(allowed_norm[1], " ")))) return(TRUE) 
  FALSE 
} 

first_last_hits <- function(au_vec, allowed_norm) { 
  au_norm <- toupper(trim_c(au_vec)); au_norm <- au_norm[au_norm != ""] 
  if (!length(au_norm)) return(c(first=FALSE, last=FALSE)) 
  c(first = au_norm[1] %in% allowed_norm, last = tail(au_norm,1) %in% allowed_norm) 
} 

# ----------------------- QUERY BUILDERS ------------------------------- 
# --- PATCH 2: tier-aware FAU/AU gate thresholds --- 
.fau_gate_pass <- function(fau_vec, last_norm, first_tok, 
                           source = c("AU","FAU"), 
                           risk_last = "STD", 
                           ad_vec = NULL, city_token = NULL, state_token = NULL, org_token = NULL, 
                           tier_context = c("FAU","AU_WITH_AFFIL","AU_NO_AFFIL")) { 
  source <- match.arg(source) 
  tier_context <- match.arg(tier_context) 
  lastU  <- toupper(last_norm %||% ""); firstU <- toupper(first_tok %||% "") 
  if (!nzchar(lastU) || !nzchar(firstU)) return(FALSE) 
  
  bf <- .best_fau_match(fau_vec, lastU, firstU) 
  if (!bf$matched) return(FALSE) 
  
  # base thresholds 
  if (identical(source, "FAU")) { 
    # T1/T2 are high-precision queries -> allow mild relaxation 
    return(bf$sim >= 0.82) 
  } 
  
  # AU: stricter, tier-aware + surname risk 
  very_common <- (lastU %in% .VERY_COMMON_LAST) || identical(risk_last, "HIGH") 
  thr <- switch( 
    tier_context, 
    AU_WITH_AFFIL = if (very_common) 0.90 else 0.86,  # balanced 
    AU_NO_AFFIL   = if (very_common) 0.96 else 0.94,  # strict 
    0.92 
  ) 
  
  # small easing if affiliation matches 
  aff <- 0 
  if (length(ad_vec)) aff <- .affil_score(ad_vec, toupper(city_token %||% ""), toupper(state_token %||% ""), toupper(org_token %||% "")) 
  if (aff >= 1.0) thr <- thr - 0.02 else if (aff >= 0.5) thr <- thr - 0.01 
  
  bf$sim >= thr 
} 
# --- PATCH 3:Last-name variants for spaced/hyphenated/particles (e.g., “EL NAJJAR”) 
.last_variants_for_query <- function(last_norm) { 
  L <- toupper(trimws(last_norm %||% "")) 
  if (!nzchar(L)) return(character(0)) 
  toks <- strsplit(L, "\\s+", perl = TRUE)[[1]] 
  base_spaced <- paste(toks, collapse = " ") 
  base_hyph   <- paste(toks, collapse = "-") 
  base_join   <- gsub("\\s+", "", base_spaced) 
  particles <- c("AL","EL","DE","DEL","DI","DA","LA","LE","VAN","VON","MC","MAC") 
  v <- c(base_spaced, base_hyph, base_join) 
  if (length(toks) == 1L) { 
    one <- toks[1] 
    for (p in particles) if (startsWith(one, p) && nchar(one) > nchar(p)+1) { 
      root <- substr(one, nchar(p)+1, nchar(one)) 
      v <- c(v, paste(p, root), paste(p, root, sep = "-")) 
    } 
  } 
  unique(v[nzchar(v)]) 
} 
# --- PATCH 5a ---FAU first-name extraction + best match (vector-safe) 
.fau_first_from <- function(fau_line, last_norm) { 
  if (is.null(fau_line) || !length(fau_line)) return(NA_character_) 
  x  <- toupper(as.character(fau_line[1] %||% "")) 
  ln <- toupper(last_norm %||% "") 
  if (!nzchar(x) || !nzchar(ln)) return(NA_character_) 
  if (!grepl(paste0("^", ln, "\\s*,"), x, perl = TRUE)) return(NA_character_) 
  right <- sub(paste0("^", ln, "\\s*,\\s*"), "", x, perl = TRUE) 
  right <- sub("\\s+[A-Z]$", "", right); right <- sub("\\s+[A-Z]$", "", right) 
  tok <- strsplit(right, "\\s+", perl = TRUE)[[1]] 
  if (!length(tok)) return(NA_character_) 
  tok[1] 
} 
# --- PATCH 5b --- 
.best_fau_match <- function(fau_vec, last_norm, target_first) { 
  lastU  <- toupper(last_norm %||% ""); firstU <- toupper(target_first %||% "") 
  if (!length(fau_vec) || !nzchar(lastU) || !nzchar(firstU)) return(list(matched = FALSE, sim = 0)) 
  cand <- fau_vec[startsWith(toupper(fau_vec), paste0(lastU, ","))] 
  if (!length(cand)) return(list(matched = FALSE, sim = 0)) 
  fn <- vapply(cand, .fau_first_from, character(1), last_norm = lastU) 
  fn <- toupper(fn[!is.na(fn) & fn != ""]) 
  if (!length(fn)) return(list(matched = FALSE, sim = 0)) 
  same_init <- substr(fn, 1, 1) == substr(firstU, 1, 1) 
  if (!any(same_init, na.rm = TRUE)) return(list(matched = FALSE, sim = 0)) 
  sims <- vapply(fn[same_init], function(f) .jw(firstU, f), numeric(1)) 
  list(matched = TRUE, sim = max(sims, na.rm = TRUE)) 
} 

quote_term <- function(x) sprintf('"%s"', trim_c(x)) 
# --- PATCH 4a: FAU builders --- 
# --- FAU builders: include inverted and natural order, prefer MI when present
build_fau_terms <- function(last_norm, first_tok, middle_tok) {
  F_name <- toupper(.sanitize_tok(first_tok %||% ""))
  M_init <- toupper(substr(.sanitize_tok(middle_tok %||% ""), 1, 1))
  lasts  <- .last_variants_for_query(last_norm)
  if (!length(lasts) || !nzchar(F_name)) return(character(0))
  out <- character(0)
  for (L in lasts) {
    # Most specific FIRST (MI present) — inverted AND natural order
    if (nzchar(M_init)) {
      out <- c(out,
               sprintf('"%s %s %s"[FAU]', L, F_name, M_init),   # inverted
               sprintf('"%s, %s %s"[FAU]', L, F_name, M_init),  # inverted with comma
               sprintf('"%s %s %s"[FAU]', F_name, L, M_init)    # natural order
      )
    }
    # Full first name, no MI — both orders
    out <- c(out,
             sprintf('"%s %s"[FAU]', L, F_name),                # inverted
             sprintf('"%s, %s"[FAU]', L, F_name),               # inverted with comma
             sprintf('"%s %s"[FAU]', F_name, L)                 # natural order
    )
  }
  unique(out)
}
# --- PATCH 4b: AU builders --- 
build_au_terms <- function(last_norm, first_tok, middle_tok) { 
  FI <- toupper(substr(.sanitize_tok(first_tok %||% ""), 1, 1)) 
  MI <- toupper(substr(.sanitize_tok(middle_tok %||% ""), 1, 1)) 
  lasts <- .last_variants_for_query(last_norm) 
  
  if (!length(lasts) || !nzchar(FI)) return(character(0)) 
  
  out <- character(0) 
  for (L in lasts) { 
    # Most specific FIRST (FI + MI) 
    if (nzchar(MI)) { 
      out <- c(out, sprintf('"%s %s %s"[AU]', L, FI, MI)) 
    } 
    # Then FI only 
    out <- c(out, sprintf('"%s %s"[AU]', L, FI)) 
  } 
  unique(out) 
}
# --- NEW: build up to two AU variants OR'd together (for T3/T4)
au_or_term_for_tiers <- function(au_terms) {
  ats <- unique(au_terms[nzchar(au_terms)])
  if (!length(ats)) return("")
  paste(head(ats, 2L), collapse = " OR ")
}
# ----------------------- FINGERPRINT & SCORING ------------------------ 
modal <- function(x) { x <- x[nzchar(x)]; if (!length(x)) return(NA_character_); tb <- sort(table(x), decreasing = TRUE); names(tb)[1] } 

extract_fau_first <- function(fau_vec, target_last) { 
  if (!length(fau_vec)) return(character(0)) 
  out <- character(0) 
  for (f in fau_vec) { 
    parts <- strsplit(f, ",", fixed = TRUE)[[1]] 
    L <- trimws(parts[1] %||% "") 
    cl <- canon_last(L); ct <- canon_last(target_last) 
    if (is.na(cl) || is.na(ct) || cl != ct) next 
    right <- trimws(parts[2] %||% "") 
    right <- sub("\\s+[A-Z]$", "", right) 
    right <- sub("\\s+[A-Z]$", "", right) 
    tok <- strsplit(right, "\\s+", perl = TRUE)[[1]] 
    if (length(tok)) out <- c(out, toupper(tok[1])) 
  } 
  out[nzchar(out)] 
} 
build_fingerprint <- function(recs, last_norm, city, state, org, K = 60L) { 
  if (!length(recs)) return(list(modal_first=NA_character_, top_coauthors=character(0), top_journals=character(0))) 
  idx <- seq_len(min(K, length(recs))) 
  fau_all <- unlist(lapply(recs[idx], function(r) r$FAU %||% character(0))) 
  ad_all  <- lapply(recs[idx], function(r) r$AD  %||% character(0)) 
  keep <- logical(length(idx)) 
  for (i in idx) { 
    ad_i <- ad_all[[i]] 
    keep[i] <- affil_score(ad_i, city, state, org) >= 0.5 
  } 
  modal_first <- modal(extract_fau_first(fau_all[keep], last_norm)) 
  aus <- unlist(lapply(recs[idx], function(r) r$AU %||% character(0))) 
  jts <- unlist(lapply(recs[idx], function(r) r$JT %||% character(0))) 
  co_tab <- sort(table(toupper(aus)), decreasing = TRUE) 
  jr_tab <- sort(table(toupper(jts)), decreasing = TRUE) 
  list(modal_first = modal_first, 
       top_coauthors = names(co_tab)[seq_len(min(10L, length(co_tab)))], 
       top_journals  = names(jr_tab)[seq_len(min(5L,  length(jr_tab)))]) 
} 
# --- PATCH 1: scalar/NA-safe helpers --- 
.nz1  <- function(x) { x <- as.character(x %||% "")[1]; isTRUE(nzchar(x)) } 
.chr1 <- function(x) { as.character(x %||% "")[1] } 
# REPLACE your .jw with this: 
.jw <- function(a, b) { 
  aa <- .chr1(a); bb <- .chr1(b) 
  if (!nzchar(aa) || !nzchar(bb)) return(0) 
  stringdist::stringsim(aa, bb, method = "jw", p = 0.1) 
} 
jw <- .jw 
# ---------- PATCH D1: scoring (bigger coauthor weight, affiliation weighted) ---------- 
score_record <- function(rec, last_norm, target_first, modal_first, city, state, org, top_co, top_jr) { 
  fau <- rec$FAU %||% character(0) 
  ad  <- rec$AD  %||% character(0) 
  jt  <- rec$JT  %||% character(0) 
  # name similarity via FAU (as you already did) 
  fns <- extract_fau_first(fau, last_norm) 
  name_sim <- 0 
  if (length(fns)) { 
    tgt <- toupper(target_first %||% "")
    if (length(fns) && nzchar(tgt)) {
      fns  <- toupper(fns %||% character())
      pool <- fns[substr(fns, 1, 1) == substr(tgt, 1, 1)]
      vals  <- if (length(pool)) vapply(pool, function(f) jw(tgt, f), numeric(1)) else numeric(0)
      name_sim <- if (length(vals)) max(vals, na.rm = TRUE) else 0
    }
  } 
  # weighted affiliation score (city=0.5, state=0.3, org=0.2) 
  aff <- affil_score(ad, city, state, org) 
  # stronger co-author overlap bonus (up to +0.10) 
  co_bonus <- 0 
  if (length(top_co) && length(rec$AU)) { 
    co_bonus <- if (any(toupper(rec$AU) %in% top_co)) 0.12 else 0 
  } 
  # slight journal familiarity bonus (up to +0.03) 
  jr_bonus <- 0 
  if (length(top_jr) && length(jt)) { 
    jr_bonus <- if (any(toupper(jt) %in% top_jr)) 0.03 else 0 
  } 
  total <- 0.55*name_sim + 0.35*aff + co_bonus + jr_bonus 
  list(total=total, name=name_sim, aff=aff) 
} 
# ----------------------- TIERED SEARCH -------------------------------- 
# --- PATCH 4a: per-row ESearch tier probe (no EFetch) --- 
probe_tiers <- function(row, retmax = 5L) {
  fts  <- build_fau_terms(row$last_name_norm, row$first_name_token, row$middle_name_token)
  ats  <- build_au_terms(row$last_name_norm, row$first_name_token, row$middle_name_token)
  
  safe_count <- function(term) {
    if (!isTRUE(nzchar(term))) return(NA_integer_)
    s <- try(esearch_pubmed(term, retmax = retmax), silent = TRUE)
    if (inherits(s, "try-error")) return(NA_integer_) else as.integer(s$count %||% 0L)
  }
  
  df   <- build_date_filter(row$EnumerationDate %||% NA_character_)
  aff  <- build_state_affil_block(row$state_token)
  
  add_df   <- function(s) if (isTRUE(nzchar(df))) paste(s, "AND", df) else s
  add_aff  <- function(base) if (nzchar(aff)) sprintf("(%s) AND %s", base, aff) else base
  
  # T1: FAU + state + date  → fallback FAU + state (no date) if 0
  t1_base <- if (length(fts)) fts[[1]] else NA_character_
  t1      <- if (length(fts)) gsub('"+','"', add_df(add_aff(t1_base))) else NA_character_
  c1      <- safe_count(t1)
  if (isTRUE(c1 == 0L) && length(fts)) {
    t1_nodf <- gsub('"+','"', add_aff(t1_base))
    c1      <- safe_count(t1_nodf)
    if (!is.na(c1) && c1 > 0L) t1 <- t1_nodf
  }
  
  # T2: FAU (no affil) + date → fallback FAU (no date) if 0
  t2_base <- if (length(fts)) fts[[1]] else NA_character_
  t2      <- if (length(fts)) gsub('"+','"', add_df(t2_base)) else NA_character_
  c2      <- safe_count(t2)
  if (isTRUE(c2 == 0L) && length(fts)) {
    t2_nodf <- gsub('"+','"', t2_base)
    c2      <- safe_count(t2_nodf)
    if (!is.na(c2) && c2 > 0L) t2 <- t2_nodf
  }
  
  # T3/T4: AU variants (unchanged)
  t3_base <- au_or_term_for_tiers(ats)
  t4_base <- au_or_term_for_tiers(ats)
  t3      <- if (nzchar(t3_base)) gsub('"+','"', add_df(add_aff(t3_base))) else NA_character_
  t4      <- if (nzchar(t4_base)) gsub('"+','"', add_df(t4_base))          else NA_character_
  c3      <- safe_count(t3)
  c4      <- safe_count(t4)
  
  list(
    t1_term = t1, t1_raw = c1,
    t2_term = t2, t2_raw = c2,
    t3_term = t3, t3_raw = c3,
    t4_term = t4, t4_raw = c4
  )
}
# --- NEW: quick debugger for zero/low-verify rows (was in driver) --- 
debug_esearch_zeros <- function(dt, limit = 3L) {
  # If called on the raw input (no outputs yet), just probe the first N rows.
  if (!("verified_n_total" %in% names(dt))) {
    message("Note: 'verified_n_total' not present; probing the first ", limit, " rows.")
    idx <- seq_len(min(limit, nrow(dt)))
    return(lapply(idx, function(i) probe_tiers(dt[i])))
  }
  # Normal mode: look at rows with zero verified and show their probes
  z <- which((dt$verified_n_total %||% 0L) == 0L)
  if (!length(z)) { message("No zero-verified rows."); return(invisible(NULL)) }
  z <- head(z, limit)
  lapply(z, function(i) probe_tiers(dt[i]))
}

# ---------- TIER RUNNER (search-and-stop; no verification in T1–T3) ----------
run_tiers_for_row <- function(row) {
  # Search-and-stop funnel:
  #  T1  FAU + STATE
  #  T2  FAU (no affil)
  #  T3  AU  + STATE
  #  T4  AU  (no affil)  [cap to 10 PMIDs]
  # Then T5 (Authors API) runs AFTER the first tier that returns ANY raw IDs.
  
  last  <- up(row$last_name_norm)
  first <- up(row$first_name_token)
  mid   <- up(row$middle_name_token)
  city  <- up(row$city_token)
  state <- up(row$state_token)
  org   <- up(row$org_token)
  
  fau_terms <- build_fau_terms(last, first, mid)
  au_terms  <- build_au_terms(last, first, mid)
  au_pair   <- au_or_term_for_tiers(au_terms)
  
  # answer scaffold (kept small & consistent)
  ans <- list(
    ok              = FALSE,
    match_tier      = NA_integer_,
    field_used      = "",
    query_fetch_primary = "",
    fetch_primary_raw_n = 0L,
    pmids_raw       = character(0),
    pmids_verified  = character(0),  # no verification for T1–T3
    pmids_firstauthor = character(0),
    pmids_lastauthor  = character(0)
  )
  
  # helper to finalize on first hit and RETURN immediately
  finalize <- function(tier_no, field_used, s, cap = NA_integer_) {
    ids <- unique(as.character(s$ids %||% character(0)))
    if (!is.na(cap)) ids <- head(ids, as.integer(cap))
    ans$ok                   <<- length(ids) > 0
    ans$match_tier           <<- as.integer(tier_no)
    ans$field_used           <<- as.character(field_used)
    ans$query_fetch_primary  <<- as.character(s$term_used %||% "")
    ans$fetch_primary_raw_n  <<- length(ids)
    ans$pmids_raw            <<- ids
    # Verification columns stay empty for T1–T3 by design
    return(ans)
  }
  
  # T1: FAU + STATE
  if (length(fau_terms)) {
    s1 <- esearch_pubmed_backoff(
      base_term  = fau_terms[[1]],
      use_affil  = TRUE,
      field_used = "FAU",
      city_token = city, state_token = state, org_token = org,
      enum_date  = row$EnumerationDate %||% NA_character_
    )
    if (isTRUE(s1$ok) && length(s1$ids)) return(finalize(1L, "FAU", s1))
  }
  
  # T2: FAU (no affil)
  if (length(fau_terms)) {
    s2 <- esearch_pubmed_backoff(
      base_term  = fau_terms[[1]],
      use_affil  = FALSE,
      field_used = "FAU",
      city_token = "", state_token = "", org_token = "",
      enum_date  = row$EnumerationDate %||% NA_character_
    )
    if (isTRUE(s2$ok) && length(s2$ids)) return(finalize(2L, "FAU", s2))
  }
  
  # T3: AU + STATE (allow up to 2 AU variants OR'd)
  if (nzchar(au_pair)) {
    s3 <- esearch_pubmed_backoff(
      base_term  = au_pair,
      use_affil  = TRUE,
      field_used = "AU",
      city_token = city, state_token = state, org_token = org,
      enum_date  = row$EnumerationDate %||% NA_character_
    )
    if (isTRUE(s3$ok) && length(s3$ids)) return(finalize(3L, "AU", s3))
  }
  
  # T4: AU (no affil) — CAP to 10 PMIDs
  if (nzchar(au_pair)) {
    s4 <- esearch_pubmed_backoff(
      base_term  = au_pair,
      use_affil  = FALSE,
      field_used = "AU",
      city_token = "", state_token = "", org_token = "",
      enum_date  = row$EnumerationDate %||% NA_character_
    )
    if (isTRUE(s4$ok) && length(s4$ids)) return(finalize(4L, "AU", s4, cap = 10L))
  }
  
  # No hits in any tier
  ans
}

# -----------------------SMOKE TEST PATCH ---------------------------------------- 
# --- NEW: unified ESearch backoff (affiliation × date) --- 
# ---------- PATCH C: backoff with robust AND formatting (STATE-ONLY AD) ----------
esearch_pubmed_backoff <- function(base_term, use_affil, field_used,
                                   city_token, state_token, org_token,
                                   enum_date,
                                   target_low = 5L, target_high = 300L) {
  
  # Enforce state-only affiliation block per strategy (ignore city/org here)
  state_block <- if (isTRUE(use_affil)) build_state_affil_block(state_token) else ""
  # label for diagnostics
  aff_label   <- if (isTRUE(use_affil) && nzchar(state_block)) "STATE_ONLY" else "none"
  
  dates <- build_date_filters(enum_date)
  
  eval_term <- function(term) {
    s <- try(esearch_pubmed(term, retmax = MAX_PMIDS_TO_FETCH), silent = TRUE)
    if (inherits(s, "try-error")) return(NULL)
    list(term_used = term,
         ids       = s$ids %||% character(0),
         count     = as.integer(s$count %||% 0L),
         raw_n     = as.integer(s$count %||% 0L),
         field_used = field_used,
         used_affil = aff_label,
         affiliation_strategy = aff_label)
  }
  
  # score: prefer counts in [target_low, target_high], prefer shorter terms
  score_of <- function(count, term_len) {
    if (count <= 0) return(-Inf)
    in_band   <- (count >= target_low && count <= target_high)
    mid       <- (target_low + target_high)/2
    closeness <- -abs(log((count + 1e-9) / mid))
    brevity   <- - (term_len / 1500)
    (if (in_band) 2 else 0) + closeness + brevity
  }
  
  # probe each date window with (name AND state?) AND date
  best <- NULL
  for (df in dates) {
    term <- .and_join(trimws(base_term), if (nzchar(state_block)) sprintf("(%s)", state_block) else "", df)
    r <- eval_term(term)
    if (!is.null(r) && r$count > 0) {
      r$._score <- score_of(r$count, nchar(term))
      if (is.null(best) || r$._score > best$._score) best <- r
    }
  }
  if (!is.null(best)) {
    best$._score <- NULL
    best$ok <- TRUE
    return(best)
  }
  
  list(ok = FALSE,
       term_used = NA_character_,
       field_used = field_used,
       used_affil = aff_label,
       affiliation_strategy = aff_label,
       ids = character(0),
       count = 0L)
}

# ----------------------- MAIN ---------------------------------------- 
# --- NEW: post-run summary (moved from driver) --- 
print_run_summary <- function(dt) { 
  cat("\n--- Final Run Summary ---\n") 
  cat("Rows processed: ", nrow(dt), "\n") 
  cat("Matched tiers (counts):\n") 
  print(table(dt$match_tier, useNA="ifany")) 
  cat("\nExamples with verified hits:\n") 
  ok <- dt[verified_n_total > 0] 
  if (nrow(ok) > 0) { 
    print(ok[1:min(5, .N), .(NPI, match_tier, verified_n_total, field_used, used_affil, affiliation_strategy, query_used)]) 
  } else { 
    cat("No verified hits found in this run.\n") 
  } 
} 
#--------------------nppes_run_pubmed_linkage---------------------------------- 
nppes_run_pubmed_linkage <- function(in_csv = IN_CSV, out_csv = OUT_CSV, 
                                     resume = TRUE, checkpoint_every = 50) { 
  
  # Resolve paths against project root 
  in_csv  <- resolve_path(in_csv,  create_parent = FALSE, must_exist = TRUE) 
  out_csv <- resolve_path(out_csv, create_parent = TRUE,  must_exist = FALSE) 
  
  # Read input safely (file=) 
  dt <- data.table::fread(file = in_csv) 
  # Columns sanity 
  needed <- c("NPI","first_name_token","middle_name_token","last_name_norm", 
              "city_token","state_token","org_token","risk_surname","EnumerationDate") 
  miss <- setdiff(needed, names(dt)) 
  if (length(miss)) stop("Missing required columns: ", paste(miss, collapse=",")) 
  if (!"pmids_t5_ultra" %in% names(dt)) dt[, pmids_t5_ultra := NA_character_]
  # Init output cols 
  out_cols_chr <- c(
    "query_used",                     # (legacy) keep for continuity; we’ll mirror query_fetch_primary
    "pmids_verified","pmids_firstauthor","pmids_lastauthor",
    "pmids_raw",                      # NEW: raw IDs from the first tier that hit (search-and-stop)
    "ambiguous_reason",
    # NEW diagnostics:
    "query_probe_best",               # first probe/backoff term that returned raw hits
    "query_fetch_primary",            # the last term we actually queried for IDs (matches search-and-stop)
    "verification_status",            # enum: Success_T1..T4, Fail_NoProbeHits, Fail_AllRejected, TimeBudget
    "tier_path",                      # e.g., T1→T1→T2→T3 (if you keep this; else NA)
    "reject_sample",                  # up to 3 rejected PMIDs with tiny reasons (name/aff/tot)
    "fingerprint_strength",           # Strong / Weak / None for the tier we scored
    "t5_seed_pmid","t5_mode","t5_endpoint","t5_reason"
  )
  out_cols_int <- c( 
    "match_tier","raw_n_total","verified_n_total","verified_n_first","verified_n_last", 
    # NEW ints: 
    "probe_best_raw_n",               # raw count for query_probe_best 
    "fetch_primary_raw_n",             # raw count for query_fetch_primary
    "t5_ultra_n"
  ) 
  for (c in out_cols_chr) if (!c %in% names(dt)) dt[, (c) := NA_character_] 
  for (c in out_cols_int) if (!c %in% names(dt)) dt[, (c) := as.integer(NA)] 
  probe_cols <- c("raw_FAU_affil","raw_FAU","raw_AU_affil","raw_AU", 
                  "term_FAU_affil","term_FAU","term_AU_affil","term_AU") 
  for (col in probe_cols) if (!col %in% names(dt)) dt[, (col) := NA_character_] 
  diag_cols <- c("field_used","used_affil","affiliation_strategy") 
  for (col in diag_cols) if (!col %in% names(dt)) dt[, (col) := NA_character_] 
  
  row_idx <- if (resume) which(is.na(dt$fetch_primary_raw_n)) else seq_len(nrow(dt))
  if (!length(row_idx)) { log_line("Nothing to do."); data.table::fwrite(dt, file = out_csv); return(invisible(dt)) } 
  
  log_line("Starting Phase-2 on", length(row_idx), "rows (of", nrow(dt), ")") 
  for (k in seq_along(row_idx)) { 
    i <- row_idx[k] 
    row <- dt[i] 
    log_line(sprintf("Row %d/%d NPI=%s", k, length(row_idx), as.character(row$NPI))) 
    
    pr <- try(probe_tiers(row, retmax = 5L), silent = TRUE) 
    if (inherits(pr, "try-error")) { 
      pr <- list(t1_raw=NA_integer_, t2_raw=NA_integer_, t3_raw=NA_integer_, t4_raw=NA_integer_, 
                 t1_term=NA_character_, t2_term=NA_character_, t3_term=NA_character_, t4_term=NA_character_) 
    } 
    
    ans <- run_tiers(row)
    
    if (inherits(ans, "try-error") || is.atomic(ans)) { 
      err <- substr(as.character(ans), 1, 200) 
      dt[i, `:=`( 
        match_tier = as.integer(NA), query_used = NA_character_, 
        raw_n_total = 0L, verified_n_total = 0L, verified_n_first = 0L, verified_n_last = 0L, 
        ambiguous_reason = paste0("Error:", err), 
        pmids_verified = "", pmids_firstauthor = "", pmids_lastauthor = "", 
        pmids_raw       = "",
        raw_FAU_affil = as.character(pr$t1_raw), 
        raw_FAU       = as.character(pr$t2_raw), 
        raw_AU_affil  = as.character(pr$t3_raw), 
        raw_AU        = as.character(pr$t4_raw), 
        term_FAU_affil = pr$t1_term, term_FAU = pr$t2_term, 
        term_AU_affil  = pr$t3_term, term_AU = pr$t4_term, 
        field_used = NA_character_, used_affil = NA_character_, affiliation_strategy = NA_character_, 
        # NEW diagnostics: make the failure rows informative too 
        query_probe_best    = NA_character_, 
        probe_best_raw_n    = as.integer(NA), 
        query_fetch_primary = NA_character_, 
        fetch_primary_raw_n = as.integer(NA), 
        verification_status = "Fail_HTTP", 
        tier_path           = NA_character_, 
        reject_sample       = "",
        fingerprint_strength = NA_character_,
        t5_ultra_n      = as.integer(ans$t5_ultra_n %||% 0L),
        pmids_t5_ultra  = paste(ans$pmids_t5_ultra %||% character(0), collapse = ";"),
        t5_seed_pmid    = ans$t5_seed_pmid %||% "",
        t5_mode         = ans$t5_mode %||% "anchor",
        t5_endpoint     = ans$t5_endpoint %||% "litsense",
        t5_reason       = ans$t5_reason %||% "skipped"
      )]
      next 
    } 
    dt[i, `:=`( 
      match_tier        = as.integer(ans$match_tier), 
      # “search-and-stop”: mirror query_fetch_primary into legacy query_used for continuity
      query_used        = as.character(ans$query_fetch_primary %||% NA_character_),
      raw_n_total       = as.integer(ans$fetch_primary_raw_n %||% NA_integer_),
      verified_n_total  = as.integer(ans$verified_total %||% 0L),
      verified_n_first  = as.integer(ans$verified_first %||% 0L),
      verified_n_last   = as.integer(ans$verified_last %||% 0L),
      ambiguous_reason  = ifelse(is.null(ans$ambiguous_reason) || !nzchar(ans$ambiguous_reason), 
                                 NA_character_, as.character(ans$ambiguous_reason)), 
      pmids_verified    = paste(ans$pmids_verified, collapse = ";"),
      pmids_firstauthor = paste(ans$pmids_firstauthor, collapse = ";"),
      pmids_lastauthor  = paste(ans$pmids_lastauthor, collapse = ";"),
      pmids_raw         = paste(ans$pmids_raw %||% character(0), collapse = ";"),
      raw_FAU_affil = as.character(pr$t1_raw), 
      raw_FAU       = as.character(pr$t2_raw), 
      raw_AU_affil  = as.character(pr$t3_raw), 
      raw_AU        = as.character(pr$t4_raw), 
      term_FAU_affil = pr$t1_term, term_FAU = pr$t2_term, 
      term_AU_affil  = pr$t3_term, term_AU = pr$t4_term, 
      field_used = as.character(ans$field_used %||% NA_character_), 
      used_affil = as.character(ans$used_affil %||% NA_character_), 
      affiliation_strategy = as.character(ans$affiliation_strategy %||% NA_character_), 
      # ---------- NEW diagnostics (always write them if present on ans) ---------- 
      query_probe_best    = as.character(ans$query_probe_best %||% NA_character_), 
      probe_best_raw_n    = as.integer(ans$probe_best_raw_n %||% NA_integer_), 
      query_fetch_primary = as.character(ans$query_fetch_primary %||% NA_character_), 
      fetch_primary_raw_n = as.integer(ans$fetch_primary_raw_n %||% NA_integer_), 
      verification_status = as.character(ans$verification_status %||% NA_character_), 
      tier_path           = as.character(ans$tier_path %||% NA_character_), 
      reject_sample       = paste(ans$reject_sample %||% character(0), collapse = ";"), 
      fingerprint_strength= as.character(ans$fingerprint_strength %||% NA_character_),
      t5_ultra_n      = as.integer(ans$t5_ultra_n %||% 0L),
      pmids_t5_ultra  = paste(ans$pmids_t5_ultra %||% character(0), collapse = ";"),
      t5_seed_pmid    = as.character(ans$t5_seed_pmid %||% ""),
      t5_mode         = as.character(ans$t5_mode %||% ""),
      t5_endpoint     = as.character(ans$t5_endpoint %||% ""),
      t5_reason       = as.character(ans$t5_reason %||% "")
    )] 
    if (k %% checkpoint_every == 0) { 
      chk_path <- sub("\\.csv$", paste0("_checkpoint_", row_idx[k], ".csv"), out_csv) 
      data.table::fwrite(dt, file = chk_path) 
      log_line("Checkpoint:", chk_path) 
    } 
  } 
  
  data.table::fwrite(dt, file = out_csv) 
  log_line("DONE →", out_csv) 
  invisible(dt) 
}
# --- self-contained entrypoint (no external driver needed) --- 
main <- function() { 
  # refresh API key / delay using current environment 
  API_KEY       <<- Sys.getenv("NCBI_API_KEY") 
  API_DELAY_SEC <<- if (nchar(API_KEY) > 0) 0.12 else 0.35 
  
  args <- commandArgs(trailingOnly = TRUE) 
  get_arg <- function(key, default = NULL) { 
    hit <- grep(paste0("^--", key, "="), args, value = TRUE) 
    if (length(hit)) sub(paste0("^--", key, "="), "", hit[1]) else default 
  } 
  flag <- function(name, default = FALSE) { 
    any(args %in% c(paste0("--", name), paste0("--", name, "=TRUE"))) || default 
  } 
  
  # Defaults point to existing folders in your project 
  in_csv  <- get_arg("in",  IN_CSV)   # e.g. "01_data_raw/....csv" 
  out_csv <- get_arg("out", OUT_CSV)  # e.g. "outputs/....csv" 
  
  # Resolve relative → absolute, create out parent 
  in_csv  <- resolve_path(in_csv,  create_parent = FALSE, must_exist = TRUE) 
  out_csv <- resolve_path(out_csv, create_parent = TRUE,  must_exist = FALSE) 
  
  resume <- !flag("no-resume") 
  chk_n  <- suppressWarnings(as.integer(get_arg("check", "50"))); if (is.na(chk_n)) chk_n <- 50L 
  dbg_n  <- suppressWarnings(as.integer(get_arg("debug", "10"))); if (is.na(dbg_n)) dbg_n <- 10L 
  
  if (!nzchar(Sys.getenv("NCBI_API_KEY", ""))) log_line("WARN: NCBI_API_KEY is not set; using slower rate limit.") 
  
  log_line("Input:", in_csv) 
  log_line("Output:", out_csv) 
  
  dt <- nppes_run_pubmed_linkage(in_csv = in_csv, out_csv = out_csv, 
                                 resume = resume, checkpoint_every = chk_n) 
  print_run_summary(dt) 
  debug_esearch_zeros(dt, limit = dbg_n) 
  invisible(dt) 
} 
cat( 
  "Loaded: 03_run_pubmed_smart_query_v2_http_EXPERT.R\n", 
  "- Tiered search (FAU+Affil → FAU → AU+Affil → AU[STD]) with 25y window\n", 
  "- Adaptive fingerprint + scoring (name 0.6, affil 0.3, bonuses 0.1)\n", 
  "- Verified != Raw by design; PMIDs & first/last author counts recorded\n", 
  "- Compact, auditable logging & checkpoints\n", 
  sep = "" 
) 
# --- autorun: run main() whether sourced or run via Rscript --- 
# comment out the next line if you only want to load the functions 
if (sys.nframe() == 0L) main() 
